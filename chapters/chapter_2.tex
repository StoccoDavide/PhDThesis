%!TEX root = ../main.tex

\chapter[Brief on DAEs]{Brief on Differential-Algebraic Equations}
\label{chap2:brief_daes}

The solution of dynamical systems represents a fundamental challenge in the fields of applied mathematics, engineering, and physical sciences. These systems, serving as mathematical representations, describe how a system evolves, offering a glimpse into phenomena ranging from chemical reactions to electrical circuit dynamics. Thereby, their solution involves determining the system's state at each time point, considering its \acp{IC} and governing equations. This process is critical for comprehending complex system behavior and predicting its evolution. However, due to their inherent complexity, solving such dynamical systems often requires sophisticated algorithms and high-performance computing resources.

While the solution of \acp{ODE} has been extensively studied and is well understood, the solution of \ac{DAE} systems is more challenging due to the presence of algebraic constraints. Specifically, \acp{DAE} is a generalization of \acp{ODE} that involves both differential and algebraic equations. They first appeared in the last century, and they gained popularity in various fields over these decades. Indeed, due to their simplicity- and straightforwardness-of-use, \acp{DAE} is a powerful tool for modeling systems with constraints~\cite{burger2018dae}, like mechanisms, electrical circuits, and chemical processes. However, on the contrary to \acp{ODE}, their solution requires specialized algorithms to obtain accurate and efficient results. In this chapter, we offer an overview of \ac{DAE} system and their state-of-the-art solution methods. Afterward, we clarify the motivation behind this research and the contributions of this thesis: The development of a novel and robust algorithm for the index reduction of generic first-order \acp{DAE} using symbolic computation methods.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Overview on Differential-Algebraic Equations}

A system of equations involving one or more unknown functions and their derivatives is referred to as a \ac{DAE} system (or \acp{DAE}). In its general form, a first-order \ac{DAE} system is expressed as
%
\begin{equation}
  \m{F}(\m{x}, \m{x}^{\prime}, t) = \m{0} \, \text{,}
  \label{chap2:eq:dae}
\end{equation}
%
where $\m{x}(t) = \m{x}$ denotes the vector of unknown functions, and $\m{F}(\m{x}, \m{x}^{\prime}, t) = \m{F}$ indicates the vector of functions consisting of $n$ components. Notice the components of $\m{x}$ and $\m{F}$ are denoted as $x_i$ and $F_i$, respectively, for $i = 1, 2, \dots, n$. Notably, the term ``\ac{DAE}'' is typically used when the highest derivative $\m{x}^{\prime}$ can not be explicitly solved in terms of the other variables $\m{x}$ and $t$ within the algebraic relationship represented by~\eqref{chap2:eq:dae}. Indeed, the Jacobian $\jac{\m{F}}{\m{x}^{\prime}}$ along a specific solution of the \ac{DAE} might become singular. Systems of equations like~\eqref{chap2:eq:dae} are also known as \emph{implicit} systems. Depending on the context, a \ac{DAE} may be employed to represent either an \ac{IVP}, if $\m{x}$ is specified at the initial time (\eg{}, $\m{x}(t_0) = \m{x}_0$), or a boundary value problem, if the solution adheres to $n$ two-point \acp{BC} (\eg{}, $\m{g}(\m{x}(t_0), \m{x}(t_f)) = \m{0}$).

Regardless of which kind of problem the specific \acp{DAE} represents, a prevalent category encountered in practical applications is the so-called \emph{semi-explicit} \ac{DAE}, represented as
%
\begin{subequations}
  \begin{empheq}[left = {\empheqlbrace}, right = {\, \text{,}}]{align}
    & \m{x}^{\prime} = \m{f}(\m{x}, \m{z}, t) \label{chap2:eq:semiexplicit_dae_diff} \\
    & \m{0} = \m{g}(\m{x}, \m{z}, t) \label{chap2:eq:semiexplicit_dae_alg}
  \end{empheq}
  \label{chap2:eq:semiexplicit_dae}% not to indent the following text
\end{subequations}
%
where~\eqref{chap2:eq:semiexplicit_dae_diff} and~\eqref{chap2:eq:semiexplicit_dae_alg} denote the \emph{differential equations} and the \emph{algebraic constraints}, respectively. An example of a \ac{DAE} system of the form~\eqref{chap2:eq:semiexplicit_dae} is the simple pendulum in redundant coordinates, where the motion of a mass at the end of a rod is constrained to a fixed length. In the Cartesian coordinates, the system is described by a set of \acp{DAE} that can be restated into the classical \ac{ODE} for a pendulum.
%
\begin{example}[The Simple Pendulum]
  Consider a straightforward example involving the motion of a pendulum in Cartesian coordinates. Let the position and velocity coordinates of the mass $m$ at the end of the rod be $(x, y)$ and $(x^{\prime}, y^{\prime}) = (u, v)$ respectively, with the angle between the rod and the vertical axis denoted as $\theta$. The position of the mass is given by the coordinates $x = \ell\sin{\theta}$ and $y = \ell\cos{\theta}$, where $\ell$ represents the pendulum length. The Euler-Lagrange equations yield the following first-order semi-explicit \ac{DAE} system
  %
  \begin{subequations}
    \begin{empheq}[left = {\empheqlbrace}, right = {\, \text{,}}]{align}
      & x^{\prime} = u \\
      & y^{\prime} = v \\
      & u^{\prime} = -\dfrac{2 \lambda x}{m} \\
      & v^{\prime} = -\dfrac{2 \lambda y}{m} - g \\
      & 0 = x^2 + y^2 - \ell^2 \label{chap2:eq:pendulum_dae_alg}
    \end{empheq}
    \label{chap2:eq:pendulum_dae}% not to indent the following text
  \end{subequations}
  %
  which corresponds to the form~\eqref{chap2:eq:semiexplicit_dae}. Here, $g$ denotes gravity, and $\lambda$ is the Lagrange multiplier. The terms $2 \lambda x$ and $2 \lambda y$ represent the constraint force maintaining the constraint~\eqref{chap2:eq:pendulum_dae_alg} and thereby ensuring the rod's fixed length. In this simple case, transforming variables $x = \ell\sin{\theta}$ and $y = \ell\cos{\theta}$, followed by algebraic manipulation, leads to the classical \ac{ODE} for a pendulum $\theta^{\prime\prime} = -g\sin{\theta}$.
\end{example}
%
It is worth noting that, while the simple pendulum's motion can be described by a simple \ac{ODE}, the \ac{DAE} system~\eqref{chap2:eq:pendulum_dae} provides a more natural representation of the system's constraints. However, in more complex scenarios, such direct transformations may not be possible, and the \ac{DAE} system must be solved as is.

\acp{DAE}, in either general or specific formulations, are commonly encountered in mathematical models across diverse engineering and scientific disciplines. Further real-life examples of \ac{DAE} systems, spanning from \ac{MB} mechanical systems to electrical circuits and \acp{TPPC}, are detailed in~\cite{brenan1995numerical}. It is important to note that while constraints in mechanical systems like the pendulum physically represent the system's limitations, those in other scenarios, such as \acp{TPPC}, may also serve as performance specifications or control objectives.

\subsection{The Significance of Differential-Algebraic Equations}

As demonstrated in the example above, working with implicit \ac{DAE} models often provides a more natural representation compared to explicit formulations, particularly when dealing with complex systems. Indeed, \ac{DAE} systems represent a generalization of \acp{ODE}. However, contrary to \acp{DAE}, \ac{ODE} systems have a well-established literature in mathematical theory and numerical solution technique. They can be typically expressed as $\m{x}^{\prime} = \m{f}(\m{x}, t)$ or $\m{M}(\m{x}, t)\m{x}^{\prime} = \m{f}(\m{x}, t)$ with $\m{M}(\m{x}, t)$ a non-singular matrix. Nonetheless, the broader scope of \acp{DAE} encompasses problems with distinct mathematical properties and presents unique challenges for numerical resolution.

To better grasp the distinction between \acp{DAE} and \acp{ODE}, let us consider this simple example
%
\begin{subequations}
  \begin{empheq}[left = {\empheqlbrace}, right = {\quad \text{,}}]{align}
  & x^{\prime} = y \label{chap2:eq:dae_example_1} \\
  & 0 = x - q \label{chap2:eq:dae_example_2}
  \end{empheq}
  \label{chap2:eq:dae_example}% not to indent the following text
\end{subequations}
%
where $q = q(t)$ is a suitably smooth function. The unique solution is $x = q$ and $y = q^{\prime}$, without the need for \acp{IC} or \acp{BC}, which implies that imposing arbitrary \acp{IC} could render the \ac{DAE} inconsistent. Additionally, unlike \acp{ODE}, the solution's dependency on the derivative of the inhomogeneous part introduces a distinctive characteristic and, even with consistent \acp{IC}, the existence and uniqueness theory for \acp{DAE} involve more intricate technical assumptions beyond mere smoothness, as seen in the \ac{ODE} case~\cite{hairer1993solving, hairer1996solving}. The requirement to differentiate $x$ in~\eqref{chap2:eq:dae_example_2}, thereby involving differentiation of the input function $q$ to determine $y$, constitutes a fundamental distinction. Hence, conversely to \acp{ODE}, \acp{DAE} may necessitate both integration and differentiation to be solved.

\subsection{The Index... Or Better Yet, the Indices}

In the realm of \acp{DAE}, the concept of index serves as a metric for measuring the deviation of a \ac{DAE} from its \emph{underlying} \ac{ODE} with \emph{invariants}. This index is a non-negative integer providing valuable insights into the mathematical structure and potential challenges associated with analyzing and solving numerically the \ac{DAE}. Generally, a higher index indicates a more complicated system, which may pose difficulties in its resolution. Various index definitions exist, these include, but are not limited to, the \emph{Kronecker} index (for linear constant coefficient \acp{DAE}), \emph{differentiation} index, \emph{structural} index, \emph{tractability} index, \emph{strangeness} index, \emph{geometric} index, and \emph{perturbation} index~\cite{mehrmann2015index}. While these indices may coincide in simple scenarios, they can differ in more intricate nonlinear and fully implicit systems~\cite{lamour2012detecting}. Furthermore, the index may exhibit local variability, assuming different values across distinct regions, and could even remain undefined at singular points~\cite{lamour2012detecting}. Notice that the differentiation index is the most common index used in practice and is typically referred to without any further epithet, \ie{}, ``the'' index. In the following sections, we delve into the differentiation, tractability, and structural indices, which are the most used in practice. Together with their qualitative definition, this discussion aims to provide a brief yet comprehensive understanding of what properties we should expect from the \acp{DAE} we are dealing with.
For this purpose, several considerations that arise from the index definitions are discussed. However, it is still a partially open problem to characterize the exact correspondence between the existing indices. The work on this topic is still ongoing~\cite{mehrmann2015index} and will not be covered in this thesis.

\paragraph{Differential Index}

Given that a \ac{DAE} encompasses both differential equations and constraints, a potential strategy involves repeatedly differentiating the constraint equations (in the semi-explicit \ac{DAE} systems of the form~\eqref{chap2:eq:semiexplicit_dae}) and substituting them into the differential ones. This process is called \emph{index reduction}, and it aims to transform the \ac{DAE} into an explicit \ac{ODE} system in all unknowns. Indeed, the solution of the \ac{DAE} system corresponds to the solution of its underlying \ac{ODE} with invariants~\cite{rheinboldt1984differential}. The number of iterations required for this transformation is termed the differential index of the \ac{DAE}, with the underlying \acp{ODE} possessing an index of 0~\cite{mehrmann2015index}.

\begin{example}[Differential Index]
  Consider the simple examples involving a given smooth function $y = y(t)$ and the unknown $x$. Then, the scalar equation
  %
  \begin{equation}
    x = y
    \label{chap2:eq:dae_index_1}
  \end{equation}
  %
  constitutes a \ac{DAE} with a differential index of 1, as it requires one differentiation to yield the \ac{ODE} $x^{\prime} = y^{\prime}$. Similarly, for the system
  %
  \begin{equation}
    \begin{system}
      x_1 &=& y \\
      x_2 &=& x_1^{\prime}
    \end{system} \quad \, \text{,}
    \label{chap2:eq:dae_index_2}
  \end{equation}
  %
  differentiating the first equation leads to $x_2 = x_1^{\prime} = y^{\prime}$, and subsequent differentiation yields $x_2^{\prime} = x_1^{\prime\prime} = y^{\prime\prime}$. Hence, this system possesses a differential index of 2, necessitating two differentiations to obtain the explicit underlying \ac{ODE} system. In both cases, the invariants are the equations that are replaced by their derivatives, \ie{}, $x = y$ for the first example, and $x_1 = y$ and $x_1^{\prime} = y^{\prime}$ for the second example.
\end{example}

It is crucial to understand that, while $n$ \acp{IC} or \acp{BC} are required to define the solution of a first-order \acp{ODE} of size $n$, the solution of the simple \acp{DAE} in the previous example is solely determined by the right-hand side, necessitating only one consistent \ac{IC}. General \ac{DAE} systems often include \ac{ODE} subsystems, resulting in $k \in [0, n]$ \acp{DOF} for the \ac{DAE} solution. However, determining which $k$ pieces of information are necessary to determine the solution can be challenging. \acp{IC} or \acp{BC} specified for the \ac{DAE} must be consistent, satisfying both the constraints and the \emph{hidden} constraints of the system, which represent the \emph{solution manifold}~\cite{rheinboldt1984differential}. For instance, in the index-1 system~\eqref{chap2:eq:dae_index_1}, an \ac{IC} must adhere to $x_1(0) = y(0)$. In the case of the index-2 system~\eqref{chap2:eq:dae_index_2}, meeting the additional constraint $x_2(0) = y^{\prime}(0)$ is necessary, along with $x_1(0) = y(0)$.

As demonstrated in the example above, the differential index indicates how far is a \ac{DAE} from an \ac{ODE} system. However, computing the differential index is not a trivial task, as it involves intensive manipulation of expressions. The differentiation process can lead to large expressions, which are inevitably computationally expensive to handle. Such limitations are particularly evident in high-index \acp{DAE}. To address this issue, further indices are introduced to provide additional understandings of the mathematical structure of \acp{DAE} without the need for extensive symbolic analysis of the system.

Although the concept of the differentiation index is widely used, it has a limitation: it is unsuitable for over- and underdetermined systems. Indeed, the differentiation index is based on a solvability concept that requires a unique solution for the \ac{DAE} system. For this reason, the \emph{strangeness index} has been introduced, which extends the differentiation index to over- and underdetermined systems. The difference between the differentiation index and the strangeness index is that the former aims at reformulating the given problem as an \ac{ODE} for uniquely solvable systems, whereas the latter aims at reformulating it as a \ac{DAE} with two parts, one part that states all constraints and another part that describes the dynamical behavior~\cite{mehrmann2015index}. Despite the strangeness index's advantages, it is not widely used in practice, and the work on this index is still ongoing. Nonetheless, we limit our study to well-determined \ac{DAE} systems.

\paragraph{Structural Index}

The structural index was initially introduced in the linear constant coefficient scenario for combinatorial analysis. Given the linear \acp{DAE} $\m{E}\m{x}^{\prime} = \m{A}\m{x} + \m{f}(t)$, the parameter-dependent pencil $(\m{E}(\m{p}), \m{A}(\m{p}))$ is constructed by substituting the nonzero elements of $\m{E}$ and $\m{A}$ with independent parameters $\m{p}$. The structural index, as discussed in~\cite{pantelides1988consistent, pryce2001simple} and expanded upon in subsequent works~\cite{benveniste2021structural}, refers to the unique integer value matching the \emph{Kronecker} index of $(\m{E}(\m{p}), \m{A}(\m{p}))$ across an open and dense subset of the parameter set. In the context of nonlinear systems, local linearization techniques are commonly applied. While it has been demonstrated in~\cite{reissig2000differential} that the differentiation index and the structural index may differ, Pantelides' algorithm, detailed in~\cite{pantelides1988consistent}, remains widely used in practical applications. Notably, in studies like~\cite{unger1995structural}, combinatorial insights are leveraged to determine which equations should be differentiated, as well as to introduce additional variables for index reduction~\cite{mattsson1993index}. However, a comprehensive assessment of the validity of this approach across various scenarios has been provided only in a few specific cases, as discussed in~\cite{mehrmann2015index}.

\paragraph{Tractability Index}

Relevant studies, such as~\cite{lamour2013differential}, reveal that the index may vary depending on a specific solution. An example is the following \ac{DAE} system.

\begin{example}[Differential Index Dependency on Solution]
  Consider the \ac{DAE} system in the semi-explicit form
  %
  \begin{equation*}
    \begin{system}
    x_1^{\prime} &=& x_3 \\
    0            &=& x_2(1 - x_2) \\
    0            &=& x_1x_2 + x_3(1 - x_2) - t
    \end{system} \, \text{.}
  \end{equation*}
  %
  The second equation admits two solutions: $x_2 = 0$ and $x_2 = 1$. If $x_2$ is continuous, the system does not transition between these values. For $x_2 = 0$, the system is semi-explicit with an index of 1, while for $x_2 = 1$, the index becomes 2. Unlike the index-1 scenario, no \ac{IC} on $x_1$ is necessary. Replacing the algebraic equation involving $x_2$ with $x_2^{\prime} = 0$, leads the index of the modified \ac{DAE} system to depend on the \ac{IC}. Specifically, if $x_2(0) = 1$, the index is 2; otherwise, it remains 1~\cite[Section 3.3]{lamour2013differential}.
\end{example}

In the definition~\eqref{chap2:eq:dae}, the function $\m{F}$ is inherently assumed to be smooth enough to calculate the derivatives. However, smoothness is not granted in practical applications. The tractability index is a concept introduced in \citet{griepentrog1986differential, marz2002index} to overcome this issue. The idea is to replace the smoothness requirements for the coefficients with the requirement for certain subspaces to be smooth. Such a concept leverages \ac{DAE} systems with \emph{properly stated leading terms}~\cite{lamour2013differential}, \ie{}, a second matrix $\m{D}(\m{x}, t)$ is used when formulating the \acp{DAE} as
%
\begin{equation*}
  \m{F}(\m{x}, \m{x}^{\prime}, t) = \m{A}(\m{x}, t) (\m{D}(\m{x}, t))^{\prime} + \m{B}(\m{x}, t) = \m{0} \, \text{.}
\end{equation*}
%
In this manner, the leading term $\m{D}(\m{x}, t)$ precisely figures out which derivatives are involved. Importantly, all admissible linearizations along the solution have the same tractability index~\cite{marz2005characterizing}.

In essence, the tractability index concept is designed to tackle the challenges posed by \acp{DAE} with multiple regular regions. It explicitly suggests that the domain of definition of the \ac{DAE} divides into maximal smooth subspaces, each delimited by singular points. Within each regular region, the \ac{DAE} exhibits a consistent structure, which can be revealed using matrix sequences formed with permissible projector functions. This construction is governed by constant rank conditions. Singular points arise when this construction process encounters difficulties. A smooth flow and reliable treatment are expected as long as the solution remains within a regular region. However, crossing or touching a boundary between regions may lead to singularities. Essentially, monitoring the structure involves computing an admissible matrix function sequence and monitoring the rank conditions to ensure the stability and reliability of the solution~\cite{lamour2011computational}. Thereby, the fundamental idea behind the tractability index concept involves the utilization of derivatives of projectors instead of derivative arrays. However, if the numerical computation of the projectors is adopted, challenges may arise in obtaining their derivatives~\cite{mehrmann2015index}.

\subsection{The Hessenberg Forms}

As will be better detailed in the forthcoming section, the implicit \ac{DAE} system~\eqref{chap2:eq:dae} may represent mathematically ill-defined problems, as well as scenarios where numerical methods fail. Thankfully, numerous high-index problems encountered in practical applications are expressed as a more restrictive formulation composed of \acp{ODE} with algebraic constraints. Within such systems, both the algebraic and differential variables are explicitly separated and identified, even for higher-index \acp{DAE}, allowing for the elimination of all algebraic variables using either index reduction, numerical direct discretization, or a combination of both. These formulations, known as \emph{Hessenberg forms} of the \acp{DAE}~\cite{brenan1995numerical}, are detailed here below and will serve as the basis for further exploration into \ac{DAE} solutions in subsequent sections and chapters.
%
\begin{itemize}
  \setlength\itemsep{0.0em}
  \item The \emph{Hessenberg index-1} \ac{DAE} system is represented as
  %
  \begin{equation*}
    \begin{system}
      \m{x}^{\prime} &=& \m{f}(\m{x}, \m{z}, t) \\
      \m{0}          &=& \m{g}(\m{x}, \m{z}, t)
    \end{system} \, \text{,}
  \end{equation*}
  %
  with $\m{x} \in \mathbb{R}^n$, $\m{z} \in \mathbb{R}^m$, $\m{f}: \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{n}$, $\m{g}: \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{m}$, and where the Jacobian $\jac{\m{g}}{\m{z}}$ is assumed non-singular for all $t$. This configuration closely resembles the semi-explicit index-1 \ac{DAE} system~\eqref{chap2:eq:semiexplicit_dae} discussed earlier. Semi-explicit index-1 \acp{DAE} shares similarities with implicit \acp{ODE}. While it is theoretically feasible to solve for $\m{z}$ in the algebraic equation (using the implicit function theorem) and then substitute it into the differential equation to derive the underlying \ac{ODE} in terms of $\m{x}$ (though uniqueness is not guaranteed), this approach is not universally recommended for numerical solutions due to potential ill-conditioning and stability issues. An example of a Hessenberg index-1 \ac{DAE} system is in the formulation of \acp{TPPC} problems~\cite{brenan1995numerical}.
  %
  \item The \emph{Hessenberg index-2} \ac{DAE} system is expressed as
  %
  \begin{equation*}
    \begin{system}
      \m{x}^{\prime} &=& \m{f}(\m{x}, \m{z}, t) \\
      \m{0}          &=& \m{g}(\m{x}, t)
    \end{system} \, \text{,}
  \end{equation*}
  %
  with $\m{x} \in \mathbb{R}^n$, $\m{z} \in \mathbb{R}^m$, $\m{f}: \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{n}$, $\m{g}: \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{m}$, and where the product of Jacobians $\jac{\m{g}}{\m{y}} \, \jac{\m{f}}{\m{z}}$ is assumed non-singular for all $t$. It is important to observe that the algebraic variable $\m{z}$ does not appear in the second equation. This arrangement characterizes a pure index-2 \ac{DAE}, where all algebraic variables act as index-2 variables. An example of Hessenberg index-2 \acp{DAE} arises from the modeling of incompressible fluid flow through discretized Navier-Stokes equations~\cite{ascher1998computer}.
  %
  \item The \emph{Hessenberg index-3} \ac{DAE} system is formulated as
  %
  \begin{equation*}
    \begin{system}
      \m{x}^\prime &=& \m{f}(\m{x}, \m{y}, \m{z}, t) \\
      \m{y}^\prime &=& \m{g}(\m{x}, \m{y}, t) \\
      \m{0}        &=& \m{h}(\m{y}, t)
    \end{system} \, \text{,}
  \end{equation*}
  %
  with $\m{x} \in \mathbb{R}^n$, $\m{z} \in \mathbb{R}^m$, $\m{f}: \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{n}$, $\m{g}: \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{m}$, $\m{h}: \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{m}$, and where the product $\jac{\m{h}}{\m{y}} \, \jac{\m{g}}{\m{x}} \, \jac{\m{f}}{\m{z}}$ is non-singular for all $t$. Determining the index of a Hessenberg \acp{DAE} follows a similar differentiation process to the general case. However, only algebraic constraints need to be differentiated~\cite{ascher1991projected}. These index-3 \ac{DAE} systems are frequently encountered in practical scenarios, notably in the fields of \ac{MBD}, \acp{TPPC}, and various engineering applications~\cite{ascher1998computer,brenan1995numerical}.
  %
  \item The \emph{Hessenberg index-($r+2$)} \ac{DAE} system is formulated as
  %
  \begin{equation*}
    \begin{system}
      \m{x}_r^\prime &=& \m{f}_r(\m{x}_1, \m{x}_2, \dots, \m{x}_r, \m{y}, \m{z}, t) \\
      &\vdots& \\
      \m{x}_2^\prime &=& \m{f}_2(\m{x}_1, \m{x}_2, \m{x}_3, \m{y}, t) \\
      \m{x}_1^\prime &=& \m{f}_1(\m{x}_1, \m{x}_2, \m{y}, t) \\
      \m{y}^\prime   &=& \m{g}(\m{x}_1, \m{y}, t) \\
      \m{0}          &=& \m{h}(\m{y}, t)
    \end{system} \quad \text{for} \quad r = 1, 2, \dots, r \, \text{,}
  \end{equation*}
  %
  with $\m{x}_i \in \mathbb{R}^{n_i}$, $\m{y} \in \mathbb{R}^m$, $\m{z} \in \mathbb{R}^m$, $\m{f}_i: \mathbb{R}^{n_i} \times \mathbb{R}^{n_{i+1}} \times \dots \times \mathbb{R}^{n_r} \times \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{n_i}$, $\m{g}: \mathbb{R}^{n_1} \times \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{m}$, $\m{h}: \mathbb{R}^{m} \times \mathbb{R} \rightarrow \mathbb{R}^{m}$, and where the product $\jac{\m{h}}{\m{y}} \, \jac{\m{g}}{\m{x}_1} \, \jac{\m{f}}{1\m{x}_2} \dots \jac{\m{f}}{r-1\m{x}_r} \, \jac{\m{f}}{r\m{z}}$ is non-singular for all $t$~\cite[Section 3.5]{lamour2013differential}.
\end{itemize}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Solution Methods for Differential-Algebraic Equations}

The literature on \ac{DAE} solution methods is vast and varied, with numerous approaches available for tackling these complex systems~\cite{brenan1995numerical, griepentrog1986differential, hairer1993solving, hairer1996solving}. Methods for solving \acp{DAE} are broadly categorized into two classes: (\emph{i}) direct discretization of the given system and (\emph{ii}) methods involving index reduction, which involves reformulating the system's equations so to ease the numerical solution process. While direct discretization is favored for its simplicity and straightforwardness, particularly with relatively low-index \acp{DAE} (typically less or equal than 3), index reduction approaches are essential to tackle higher-index \acp{DAE}. It is important to note that these two methods can be used alongside, by applying first index reduction to come up with an index-1 or index-2 \ac{DAE} system, and secondly by applying a numerical direct discretization. As we will see, this approach keeps expressions' complexity low while exploiting the straightforwardness of direct discretization. Nonetheless, there exist some classes of Radau and \acp{BVM} that allow for the direct discretization of very high-index \acp{DAE} without the need for index reduction~\cite{martinvaquero2010radau, amodio1993boundary, amodio1997parallel, amodio1998algorithm}.

\subsection{Numerical Direct Discretization}

Direct discretization is preferred due to its simplicity and computational efficiency, as index reduction methods can be time-consuming and computationally expensive, often requiring more user inputs and interventions. However, direct discretization is most effective for index-1, index-2, and index-3 Hessenberg \ac{DAE} systems. Usually, many practical \acp{DAE} fall into these categories or can be transformed into simple combinations of Hessenberg systems. Despite this, challenges may still arise, and even for these restricted classes of problems, direct application of numerical \ac{ODE} methods may lead to insufficient results and instabilities. For \acp{DAE} with an index greater than 2, employing index reduction techniques to solve the problem in a lower-index form is often considered the most effective approach. Similar considerations arise in cases resembling singularly perturbed \ac{ODE} systems, such as
%
\begin{equation}
  \begin{system}
  \m{y}^{\prime}             &=& \m{f}(\m{y}, \m{z}, t) \\
  \varepsilon \m{z}^{\prime} &=& \m{g}(\m{y}, \m{z}, t)
  \end{system} \, \text{,}
  \label{chap2:eq:singularly_perturbed}
\end{equation}
%
where $\varepsilon$ is a small perturbation parameter. Setting $\varepsilon = 0$ reduces~\eqref{chap2:eq:singularly_perturbed} to the \acp{DAE}~\eqref{chap2:eq:semiexplicit_dae}. In general, due to the system's stiffness for small $\varepsilon$, methods designed for such \ac{ODE} system, like \ac{BDF} and Radau collocation methods, are natural choices for directly discretizing implicit \acp{DAE} of the form~\eqref{chap2:eq:dae}~\cite{martinvaquero2010radau}.

\paragraph{Backward Euler Method}

The concept of direct discretization is quite straightforward: it involves approximating $\m{x}$ and $\m{x}^{\prime}$ using a discretization technique, such as multistep methods or \ac{RK} methods. As a first example, let us take the Backward Euler method, which is the simplest method exhibiting a stiff decay property. Applying the Backward Euler formula to $\m{x}^{\prime}$ in~\eqref{chap2:eq:dae}, we obtain a system of $N$ nonlinear equations
%
\begin{equation}
  \m{F}\left(\m{x}_n, \dfrac{\m{x}_n - \m{x}_{n-1}}{h_n}, t_n\right) = \m{0} \quad \text{for} \quad n = 1, 2, \dots, N \, \text{.}
  \label{chap2:eq:backward_euler}
\end{equation}
%
Here, $\m{x}_n$ represents the approximation of $\m{x}(t_n)$, $h_n = t_n - t_{n-1}$ denotes the time step, $t_n$ are the time points, and $N$ is the total number of time points. Solving this nonlinear equation system recursively provides a numerical solution for~\eqref{chap2:eq:dae}.

This method is effective for index-1 \acp{DAE} and particularly suitable for stiff index-1 \acp{DAE} and stiff \acp{ODE}. However, for higher-index \acp{DAE} this straightforward approach may not suffice. In some cases, even seemingly simple higher-index \ac{DAE} systems with well-defined and stable solutions can pose challenges, making methods like the Backward Euler method, as well as other multistep and \ac{RK} methods, unstable or inapplicable~\cite{ascher1998computer}. Practical difficulties may arise during the resolution of the nonlinear system~\eqref{chap2:eq:backward_euler} for $\m{x}_n$ given $\m{x}_{n-1}$, where iterative numerical methods like the Newton method are employed. Due to these implementation challenges, direct discretization of fully implicit \acp{DAE} with an index greater than 1 is generally discouraged. Despite this, for fully implicit index-1 and semi-explicit index-2 \acp{DAE}, the Backward Euler method is stable, convergent, and first-order accurate~\cite{brenan1995numerical, hairer1999stiff}.

\paragraph{Backward Differentiation Formula and Linear Multistep Methods}

Although Euler is a first-order method, achieving higher accuracy without reducing step size requires the use of higher-order methods. One such method is the constant step-size \ac{BDF}, which is applied to a general nonlinear \ac{DAE} system of the form~\eqref{chap2:eq:dae} and is given by
%
\begin{equation*}
  \m{F}\left(\m{x}_n, \dfrac{1}{\beta_0 h} \sum_{j=0}^{s} \alpha_j \m{x}_{n-j}, t_n\right) = \m{0} \quad \text{for} \quad n = 1, 2, \dots, N \, \text{,}
\end{equation*}
%
where $\beta_0$ and $\alpha_j$ for $j = 0, 1, \dots, s$ are the coefficients of the \ac{BDF} method. The $s$-step \ac{BDF} method of fixed step size $h$ is convergent of order $\mathcal{O}(h^s)$ under certain conditions. Similar convergence results have been established for general linear multistep methods, provided their coefficients satisfy a set of order conditions, including those unique to \acp{DAE}~\cite{brenan1995numerical}. \ac{BDF} methods meet these additional requirements (refer to~\cite{brenan1995numerical} for additional information).

\paragraph{Radau Collocation and Implicit Runge-Kutta Methods}

The $s$-stage implicit \ac{RK} method, when applied to a general nonlinear \ac{DAE} of the form~\eqref{chap2:eq:dae}, is expressed as
%
\begin{equation*}
  \begin{array}{l}
    \m{F}\left(\m{x}_{k} + h_k\displaystyle\sum_{j=1}^{s}a_{ij} \m{K}_j, \m{K}_i, t_{k} + c_i h_k\right) = \m{0} \, \text{,} \\[1em]
    \m{x}_{k+1} = \m{x}_{k} + h_k\displaystyle\sum_{i=1}^{s} b_i \m{K}_i \, \text{,}
  \end{array}
\end{equation*}
%
where $c_i$, $a_{ij}$, $b_i$ for $i = 1, 2, \dots, s$ and $j = 1, 2, \dots, s$ are the coefficients of the \ac{RK} method, with the additional assumption that the matrix $\m{A} = (a_{ij})$ is non-singular. Similarly, the $s$-stage implicit \ac{RK} method for semi-explicit \acp{DAE} of the form~\eqref{chap2:eq:semiexplicit_dae} is given by
%
\begin{equation*}
  \begin{array}{l}
    \m{K}_i = \m{f}\left(\m{x}_{k} + h_k\displaystyle\sum_{j=1}^{s}a_{ij} \m{K}_j, \m{z}_{k}, t_{k} + c_i h_k\right) = \m{0} \, \text{,} \\[1em]
    \m{x}_{k+1} = \m{x}_{k} + h_k\displaystyle\sum_{i=1}^{s} b_i \m{K}_i \, \text{,} \\[1.75em]
    \m{0} = \m{g}\left(\m{x}_{k}, \m{z}_{k}, t_{k} + c_i h_k\right) \, \text{,}
  \end{array}
\end{equation*}
%
for $i = 1, 2, \dots, s$. Notice that it is possible to avoid the quadrature step for the algebraic variables $\m{z}$ by employing stiffly accurate methods, \ie{}, \ac{RK} methods satisfying $b_i = a_{si}$ for $i = 1, 2, \dots, s$, like the Radau collocation methods~\cite{martinvaquero2010radau, brenan1995numerical}.

\vspace{1.0em}

Implementing direct discretization methods for \acp{DAE} faces practical challenges. These challenges encompass obtaining a consistent set of \acp{IC}, addressing the ill-conditioning of the iteration Jacobian matrix, and managing error estimation for step-size control. However, for specific classes of \acp{DAE}, such as semi-explicit ones in Hessenberg form and \acp{ODE} with hidden constraints typically found in \ac{MB} mechanics, highly efficient and robust numerical methods called \emph{stabilized} or \emph{projected methods} exist. The core approach involves discretizing the differential equations using a suitable numerical \ac{ODE} method with a stabilization technique or coordinate projection step to align the numerical solution with the constraints~\cite{eichsoellner1998numerical}. For a thorough understanding of numerical aspects concerning \acp{DAE}, refer to~\cite{ascher1998computer, brenan1995numerical, hairer1999stiff}.

\subsubsection[Software for DAEs Numerical Solution]{Software for Differential-Algebraic Equations Numerical Solution}

Several software packages are available for solving \acp{DAE}, including both general-purpose and specialized tools. Some of the most widely used software packages for \ac{IVP} and boundary value problem \acp{DAE}'s numerical solutions are listed here below.
%
\begin{itemize}
  \setlength{\itemsep}{0.0em}
  \item \textbf{\textsc{Dassl}}, developed by Linda Petzold, utilizes \ac{BDF} formulas to solve generic index-1 \acp{DAE}. Variants tailored for large-scale problems and sensitivity analysis are also available. Specifically, \textsc{Daspk}, a later iteration of \textsc{Dassl}, can handle large Hessenberg index-2 \acp{DAE}; \textsc{Daspkadjoint}, on the other hand, implements the adjoint method for sensitivity analysis of \ac{DAE} systems~\cite{brenan1995numerical}.
  \item \textbf{\textsc{Radau5}} is a software by Ernst Hairer and Gerhard Wanner based on the 3-stage Radau collocation method~\cite{hairer1999stiff}. It tackles problems expressed as $\m{M}\m{x}^{\prime} = \m{f}(\m{x}, t)$, where $\m{M}$ is a constant, possibly singular, square matrix. \textsc{Radau5} solves \acp{DAE} up to index 3, with the identification of higher-index variables being required from the user. Higher-order Radau methods are available, but they require more intricate and custom implementations~\cite{martinvaquero2010radau}.
  \item \textbf{\textsc{Ida}} is a submodule of the \ac{SUNDIALS} software package developed by Radu Serban and Alan Hindmarsh at \ac{LLNL}~\cite{hindmarsh2005sundials, gardner2022sundials}. \textsc{Ida} is coded in \cc{} and tackles nonlinear \acp{DAE}. It is derived from the aforementioned \Fortran{} package \textsc{Daspk}. Additionally, a related code for solving \acp{ODE} with invariants is available under the name of \textsc{Cpodes}.
  \item \textbf{\textsc{Daepack}}, is a software library created by Paul Barton and his team at \ac{MIT}~\cite{tolsma2000daepack}. While its name stands for \ac{DAE} Package, \textsc{Daepack}'s capabilities extend beyond \ac{DAE} analysis, encompassing both symbolic and numerical components for modeling and general numerical computations.
  \item \textbf{\textsc{Coldae}} was firstly presented in~\cite{bader1987new}. It employs projected Gauss collocation to solve boundary value problems for semi-explicit \acp{DAE} with an index of at most 2.
\end{itemize}

\subsection{Index Reduction Methods}
\label{chap2:sec:index_reduction_methods}

For higher-index \acp{DAE}, direct discretization methods might not offer the most efficient solution. In such scenarios, index reduction methods are preferred. Such methods involve reformulating the \ac{DAE} into a lower-index form, which can be tackled using direct discretization methods or standard techniques for \ac{ODE} integration, aided by projection or stabilization techniques. Specifically, the index is reduced by differentiating the algebraic constraints and substituting them into the differential equations. This process repeats until the \ac{DAE} transforms into an explicit \ac{ODE} system. As mentioned earlier, the number of iterations needed for this transformation is termed the index of the \ac{DAE}, with \acp{ODE} having an index of 0.

We have seen that there exist multiple definitions of the index of a \ac{DAE} system, and each of them is tailored to a particular index reduction technique. Consequently, there are various approaches to index reduction, whose characteristics also depend on factors like the capabilities of the available package, the user's expertise, and the specific problem at hand. Each method has advantages and disadvantages, being more or less suitable for different types of \acp{DAE} and varying in computational cost, implementation requirements, and theoretical foundations.

\paragraph{Differential Index Reduction}

As expert readers may notice, the differential index reduction method bears close relation to symbolic computation and symbolic computation software such as \Maple{}, \Mathematica{}, and \Python{}'s library \textsc{SymPy}. These prove invaluable in executing the differentiation and substitution steps essential for reducing the differential index of a \ac{DAE} system. These tools also facilitate the derivation of the \acp{DAE}' Jacobian, crucial for numerical solution approaches. However, in some problems (very few), symbolic computation tools can also furnish the \acp{DAE}' solution, aiding in the validation and assessment of numerical solutions obtained through numerical methods. Nevertheless, it is worth noting that symbolic computation tools are not always the optimal choice for \acp{DAE} resolution, particularly for large-scale or intricate systems. Indeed, the computational overhead of symbolic computation tools may become prohibitive in such cases, prompting a preference for a mixed approach involving both symbolic computation and numerical methods.

As previously mentioned, the basic idea behind the differential index reduction method is to differentiate the algebraic constraints and substitute them into the system. This process repeats until the \ac{DAE} system transforms into an explicit \ac{ODE} system. To do so, one can indiscriminately differentiate all equations in the \ac{DAE} system. However, this approach is not the most efficient. In some cases, differentiating only a subset of the equations leads to a more straightforward and less computationally expensive solution, especially for large-scale \acp{DAE}. Indeed, algebraic equations should be only differentiated. To this end, the system should be restated into its Hessenberg form, where the algebraic constraints are explicitly separated from the differential equations~\cite{shmoylova2013simplification}. Thereby, the isolation process of algebraic equations is crucial for the successful application of the differential index reduction method.

\paragraph{Structural Index Reduction}

The most common index reduction methods are those based on the \emph{Pantelides algorithm}~\cite{pantelides1988consistent}, which fall under the umbrella of the \ac{SA} methods. However, recent advances have led to the development of more effective algorithms, such as S. Campbell and W. Gear's method of \emph{differential arrays}~\cite{campbell1995index} and J. Pryce's \emph{$\Sigma$-method}~\cite{pryce1998solving}. This latter method provides a systematic approach to index reduction and the solution of \acp{DAE} using the Taylor series. Nonetheless, it has been proven to be a generalization of the Pantelides algorithm~\cite{pryce2001simple}.

The \ac{SA} examines the regularity or singularity of \ac{DAE} in a generic sense. This analysis is typically conducted through a simplified representation of the system's structure using tools such as the bipartite graph, which characterizes the system's incidence matrix non-zeros pattern. Structural regularity or singularity, as well as related properties, are then assessed based on this graph. This approach is effective for sparse systems, where the incidence matrices have a small proportion of non-zero entries. \ac{SA} techniques also extend to numerical regularity, which is determined by the system's Jacobian matrix. Understanding the link between numerical regularity and structural regularity is crucial for correctly applying \ac{SA} methods. Additionally, the \ac{SA} of systems of equations can be extended to include systems with existential quantifiers~\cite{benveniste2021structural}.

In essence, the \ac{SA} of \ac{DAE} systems aims to address the following problem. Considering a \ac{DAE} system represented by equation~\eqref{chap2:eq:dae} as a set of algebraic equations with the leading variables as unknowns, the goal is to determine if this system is structurally non-singular. If it is, then given values for all the derivatives $x_k^{\prime}$ for $i = 1, 2, \dots, n$ and $k = 0, 1, \dots, d_{i-1}$, a unique leading value for the $i$ variables can be computed structurally and, in other words, the system behaves like a \ac{ODE} system. However, if the system is not structurally non-singular, additional latent equations can be derived by suitably differentiating selected equations. This process transforms the system into a \ac{ODE}-like form without altering its solution set~\cite{benveniste2021structural}.

The structural index introduced earlier serves as a metric for quantifying the number of times the system needs to be differentiated to attain a structurally non-singular form. It is important to emphasize that the structural index can be significantly higher than the differential index. In particular, \citet{reissig2000differential} highlights that the structural index of constant coefficients linear \acp{DAE}, with a differential index of 1, can be arbitrarily high, which contrasts many previous findings in the literature. This reveals that applying Pantelides' algorithm to index-1 \acp{DAE} may lead to an indefinite number of iterations and differentiations.

\paragraph{Tractability Index Reduction}

Another approach to index reduction is the so-called \emph{projector-based analysis}, thoroughly detailed in \citet{lamour2013differential, marz2014differential}. The projector-based analysis, founded on the tractability index, explores the concept that complex \acp{DAE} can consist of distinct regularity regions bordered by critical points. These regions reveal a consistent structure that can be uncovered using matrix function sequences formed with admissible projector functions and guided by constant rank conditions. Smooth solutions are expected within these regions, but crossing borders may lead to singularities. In essence, this method provides a way of tracking the system's structure by computing an admissible matrix function sequence and monitoring rank conditions~\cite{lamour2011computational}.

More specifically, the projector-based analysis involves finding a projector that separates the differential and algebraic components of a \ac{DAE} system with properly stated leading term~\cite{lamour2011computational}. Hence, the projector transforms the \ac{DAE} system into a lower-index form, which is then integrated with improved numerical stability (thanks to the properly stated leading term)~\cite{higueras2004differential}. Notice that in the case of nonlinear \acp{DAE}, the projector-based analysis is typically performed pointwise, which means that the algorithmic steps of the computation of admissible null-space projectors are performed at each time step, ensuring that the changes in the \acp{DAE} structure are accounted for at each time step. Special continuous projector-valued functions can be reutilized for multiple time steps, thus reducing the computational cost of the method~\cite{lamour2012detecting}.

\subsection{Software for Index Reduction}

Following this brief introduction to the most widely recognized index reduction algorithms, we provide an overview of the specific index reduction algorithms found in current state-of-the-art software. The following list is limited to the most prominent solutions dedicated to dynamic system modeling and simulation that are adopted in both academia and industry.
%
\begin{itemize}
  \setlength{\itemsep}{0.0em}
  \item \textbf{\Matlab{}} uses the Pantelides algorithm~\cite{pantelides1988consistent} to reduce the \acp{DAE} to index-1. Alternatively, or if the latter fails, the more reliable but slower Gaussian elimination algorithm is employed to obtain an index-0 \ac{DAE} system~\cite{matlab}.
  \item \textbf{\Modelica{}}~\cite{mattsson1997modelica, mattsson1998physical}, \Modelica{}-based software and \textbf{\ModelingToolkit{}}~\cite{modelingtoolkit} employ the Pantelides algorithm~\cite{pantelides1988consistent} along with the dummy derivatives method~\cite{mattsson1993index} to automatically perform the reduction to index-1 \acp{DAE}.
  \item \textbf{\Mathematica{}} offers a comprehensive suite of index reduction algorithms~\cite{mathematica}. It can reduce the index of \acp{DAE} using the Pantelides~\cite{pantelides1988consistent} or structural matrix~\cite{unger1995structural, chowdhry2004symbolic} methods. Additionally, it implements dummy derivatives~\cite{mattsson1993index} and projection methods for taking hidden constraints into account during numerical integration.
  \item \textbf{\Maple{}} performs symbolic index reduction within the \texttt{dsolve} function~\cite{maple}. However, the implemented algorithms are not documented or referenced. They are likely to be based on the projection method outlined in~\cite{shmoylova2013simplification}. Notably, the patents by the same authors~\cite{postma2012exact, shmoylova2012method, postma2015exact} discuss techniques for eliminating isolated parameters, extracting parameter sub-expressions from \acp{DAE}, and establishing minimal disconnected clusters of parameter sub-expressions. We would like to point out that this information is to be taken with caution, as \Maple{} does not provide specific references on this topic.
  \item \textbf{\textsc{Daesa}}, developed by Guangning Tan and Ned Nedialkov in collaboration with John Pryce, is a \Matlab{} toolbox designed for conducting \ac{SA} of \acp{DAE}. \textsc{Daesa} can analyze fully nonlinear systems, irrespective of their order or index, making it capable of determining crucial attributes such as the structural index, \acp{DOF}, constraints, and variables requiring initialization, while also proposing a suitable solution strategy~\cite{nedialkov2015algorithm, tan2016symbolic}. Additionally, it can generate a block-triangular form of the \acp{DAE}, enabling efficient block-wise solution strategies.
  \item \textbf{\textsc{Daets}}, short for \acp{DAE} by Taylor Series, is a \cpp{} package developed by Guangning Tan and Ned Nedialkov in collaboration with John Pryce~\cite{nedialkov2007solvingI, nedialkov2007solvingII, nedialkov2008solvingIII}. It is designed for tackling \acp{IVP} associated with \ac{DAE} systems. \textsc{Daets} leverages Pryce's method for the \ac{SA} of \acp{DAE}, providing a powerful means to determine the system's index, \acp{DOF}, and to precisely identify which components necessitate \acp{IC}.
  \item \textbf{\textsc{Initdae}} is a \Python{} prototype designed to calculate consistent \acp{IC} for \acp{DAE}, determining their index, as well as a condition number that aids in identifying singularities. The initialization algorithm utilizes a constrained optimization approach based on projectors, with \ac{AD}. Local structural characteristics of the \ac{DAE} system are examined through the \ac{SVD}~\cite{estvezschwarz2021initdae}
\end{itemize}
%
All the showcased software solutions offer integrators for index-0 and index-1 \acp{DAE}. Depending on the system's stiffness, users can select the most appropriate algorithm for numerically integrating the reduced-index system.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Advancing Index Reduction}

Notably, in findings presented by the group of Roswitha M{\"a}rz, which are summarized in~\cite{lamour2013differential}, there is little mention of symbolic computation tools in the derivation of projector functions and matrix function sequences. Instead, the authors rely on numerical factorization methods with a coupled automatic differentiation method to compute the projector functions and the matrix function sequences~\cite{lamour2011computational, schwarz2015diagnosis}. However, it is noteworthy that in \citet{lamour2013differential}, authors show that the latest state-of-the-art \ac{CAS} allows the derivation of smooth symbolic projector functions, especially for simple or small-scale \acp{DAE}. This can significantly reduce the computational cost of the projector-based analysis method as it eliminates the need for numerical factorization methods and automatic differentiation.

Nevertheless, the most relevant consideration arising from what we stated in Section~\ref{chap2:sec:index_reduction_methods}, as well as in \citet{bojarincev1980regular, gear1984ode, griepentrog1989basic} (despite being these works focused on linear \acp{DAE}), is that a neat separation between the differential and algebraic equations of the \ac{DAE} system is crucial for the successful application of the differential index reduction method. This separation is achieved by transforming the \ac{DAE} system into its Hessenberg form. However, this process is not always straightforward. Particularly, for generic nonlinear \acp{DAE}, the separation process can be computationally expensive and time-consuming.

\paragraph{A Novel Symbolic Algorithm for Index Reduction}

Starting from these observations, we recall and further specify the primary objective of this thesis: The development of an index reduction algorithm for \acp{DAE} based on symbolic matrix factorization techniques. It is worth noting that the algorithm is neither directly based on the projector-based analysis nor in index concepts other than the differential index. The algorithm is limited to generic well-determined \acp{DAE} of the form~\eqref{chap2:eq:dae}, linear in the states' derivatives. Such a reduction algorithm is implemented as an open-source \Maple{} package. On the other hand, the numerical integration is performed by an open-source \Matlab{} toolbox. Both software constitutes the \Indigo{} toolbox~\cite{indigo}, whose dependencies are the symbolic linear algebra package \LAST{}~\cite{last} and the large expression management package \LEM{}~\cite{lem}. The proposed symbolic algorithm and its dedicated numerical scheme are validated through a set of benchmark \acp{DAE} from the literature arising in various fields, including problems from \ac{MBD}, electrical circuit simulation, and \ac{TPPC}. In the forthcoming chapters, we will build up the necessary theoretical background and tools to understand the algorithm's inner workings.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
