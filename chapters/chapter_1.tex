%!TEX root = ../main.tex

\chapter{Introduction}
\label{chap1:introduction}

The resolution of dynamical systems stands as a fundamental challenge in fields like applied mathematics, engineering, and the physical sciences. These systems, serving as mathematical representations, describe how a system evolves, offering insights into phenomena ranging from celestial motion to electrical circuit dynamics. Resolving dynamical systems involves determining the system's state at each time point, considering its initial conditions and the governing equations of motion. This process is critical for comprehending complex system behavior and forecasting future evolution. However, in many cases, solving dynamical systems often requires sophisticated algorithms and high-performance computing resources due to their inherent complexity.


Typically, dynamical systems are described by \acp{ODE}, \acp{DAE}, or \acp{PDE}. While the solution of \acp{ODE} and \acp{PDE} has been extensively studied and is well understood, the solution of \acp{DAE} (and \acp{PDAE}) poses more challenges due to the presence of algebraic constraints. Specifically, \ac{DAE} systems are a generalization of \acp{ODE} that involve both differential and algebraic equations. Over the last century, new modeling techniques have been developed to describe complex systems, leading to the formulation of \acp{DAE}. These equations are preferred for modeling systems with constraints, like mechanical systems, electrical circuits, and chemical processes, due to their simplicity and ability to describe system behavior. However, solving \acp{DAE} is more complex than \acp{ODE}, mainly because of the presence of algebraic constraints that require to be satisfied at each time point. For this reason, specialized algorithms are needed to solve \acp{DAE} accurately and efficiently. In this chapter, we offer an overview of \ac{DAE} system and their solution methods. Moreover, we will introduce a novel approach to address this challenge, highlighting the crucial role of computer algebra in the process.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Introduction to Differential-Algebraic Equations}

A system of equations involving one or more unknown functions and their derivatives is referred to as a \acp{DAE}. In its general form, a first-order \ac{DAE} system is expressed as
%
\begin{equation}
  \m{F}(\m{x}, \m{x}^{\prime}, t) = \m{0} \, \text{,}
  \label{chap1:eq:dae}
\end{equation}
%
where $\m{x} = \m{x}(t)$ denotes the vector of unknown functions and $\m{F} = \m{F}(\m{x}, \m{x}^{\prime}, t)$ consists of $n$ components, denoted as $x_i$ and $F_i$, respectively, for $i = 1, 2, \dots, n$. The term ``\ac{DAE}'' is typically used when the highest derivative $\m{x}^{\prime}$ cannot be explicitly solved in terms of the other variables $\m{x}$ and $t$ within the algebraic relationship represented by~\eqref{chap1:eq:dae}. Notably, the Jacobian $\m{JF}_{\m{x}^{\prime}}$ along a specific solution of the \ac{DAE} might become singular. Systems of equations like~\eqref{chap1:eq:dae} are also known as \emph{implicit} systems. Depending on the context, a \ac{DAE} may represent either an \ac{IVP}, where $\m{x}$ is specified at the initial time ($x(t_0) = x_0$), or a \ac{BVP}, where the solution adheres to $n$ two-point boundary conditions $g(x(t_0), x(t_f)) = 0$.

The solution approach for a \ac{DAE} depends on its structural characteristics. A significant category of \acp{DAE} encountered in practical applications is the semi-explicit \ac{DAE}, represented as
%
\begin{equation}
  \begin{cases}
    \m{y}^{\prime} = \m{f}(\m{y}, \m{z}, t) \\
    \m{0} = \m{g}(\m{y}, \m{z}, t)
  \end{cases} \, \text{,}
  \label{chap1:eq:semiexplicit_dae}
\end{equation}
%
where $\m{0} = \m{g}(\m{y}, \m{z}, t)$ denotes the \emph{algebraic constraints}. An example of a \ac{DAE} system of the form~\eqref{chap1:eq:semiexplicit_dae} is the simple pendulum, where the motion of a mass at the end of a rod is constrained to a fixed length. In the Cartesian coordinates, the system is described by a set of \acp{DAE} that can be transformed into a classical \ac{ODE} for a pendulum.
%
\begin{example}[The Simple Pendulum]
  Consider a straightforward example involving the motion of a pendulum in Cartesian coordinates (see Figure 1). Let the position and velocity coordinates of the mass $m$ at the end of the rod be $(x, y)$ and $(x^{\prime}, y^{\prime}) = (u, v)$ respectively, with the angle between the rod and the vertical axis denoted as $\theta$. The position of the mass is given by $x = \ell\sin{\theta}$ and $y = \ell\cos{\theta}$, where $\ell$ represents the length of the pendulum. The Euler-Lagrange equations yield the following first-order semi-explicit \ac{DAE} system
  %
  \begin{equation*}
    \begin{cases}
      x^{\prime} = u \\
      y^{\prime} = v \\
      u^{\prime} = -\dfrac{2 \lambda x}{m} \\
      v^{\prime} = -\dfrac{2 \lambda y}{m} - g
      0 = x^2 + y^2 - \ell^2
    \end{cases} \, \text{,}
  \end{equation*}
  %
  which corresponds to the form~\eqref{chap1:eq:semiexplicit_dae}. Here, $g$ denotes gravity, $\lambda$ is the Lagrange multiplier, and $\ell$ represents the length of the pendulum. The terms $2 \lambda x$ and $2 \lambda y$ represent the constraint force maintaining $x^2 + y^2 = \ell^2$, ensuring the rod's fixed length. In this simple case, transforming variables $x = \ell\sin{\theta}$ and $y = \ell\cos{\theta}$, followed by algebraic manipulation, leads to the classical \ac{ODE} for a pendulum $\theta^{\prime\prime} = -g\sin{\theta}$. However, in more complex scenarios, such simplifications may not be feasible.
\end{example}
%
\acp{DAE}, in both their general and specific formulations, are commonly encountered in mathematical models across engineering and scientific disciplines. Further real-life examples of \ac{DAE} systems, spanning from \ac{MB} mechanical systems to electrical circuits and \acp{TPPC}, are detailed in~\cite{brenan1995numerical}. It is important to note that while constraints in mechanical systems like the pendulum are physical, those in other scenarios, such as \acp{TPPC}, serve as performance or control specifications rather than physical limitations.

\subsection{The Significance of Differential-Algebraic Equations}

As mentioned previously, \ac{DAE} systems represent a generalization of \acp{ODE}, typically expressed as $\m{x}^{\prime} = \m{f}(\m{x}, t)$, which have a well-established literature in both mathematical theory and numerical solution. While \acp{ODE} can be viewed as a subset of \acp{DAE}, the broader scope of \acp{DAE} encompasses problems with distinct mathematical properties and presents unique challenges for numerical resolution. Implicit \ac{DAE} models, as demonstrated in the example above, often provide a more natural representation compared to explicit formulations. Consequently, working with \ac{DAE} models is highly advantageous, particularly when dealing with complex systems.

To grasp the distinction between \acp{DAE} and \acp{ODE}, let us consider the simple example
%
\begin{equation*}
  \begin{cases}
  y^{\prime} = z \\
  0 = y - q(t)
  \end{cases} \, \text{,}
\end{equation*}
%
where $q(t)$ is a suitably smooth function. Clearly, the unique solution is $y = q(t)$ and $z = q^{\prime}(t)$, without the need for initial or boundary conditions. This implies that imposing arbitrary initial conditions could render the \ac{DAE} inconsistent. Additionally, unlike \acp{ODE}, the solution's dependency on the derivative of the inhomogeneous part introduces a distinctive characteristic. Moreover, even with consistent initial values, the existence and uniqueness theory for \acp{DAE} involve more intricate technical assumptions beyond mere smoothness, as seen in the \ac{ODE} case. The requirement to differentiate $y$ in the first equation of this example, thereby involving differentiation of the input function $q$ to determine $z$, constitutes a fundamental distinction. Hence, unlike the case of \acp{ODE}, \acp{DAE} may necessitate both integrations and differentiations to be resolved.

\subsection{The Index and Mathematical Structure}

In the realm of \acp{DAE}, the concept of index serves as a metric for measuring the deviation of a \ac{DAE} from its corresponding \ac{ODE}. This index is a non-negative integer that provides valuable insights into the mathematical structure and potential complexities associated with analyzing and numerically solving the \ac{DAE}. Generally, a higher index indicates increased challenges in numerical resolution. Various index definitions exist, including the \emph{Kronecker} index (for linear constant coefficient \acp{DAE}), \emph{differentiation} index, \emph{structural} index, \emph{tractability} index, \emph{strangeness} index, \emph{geometric} index, and \emph{perturbation} index~\cite{mehrmann2015index}. While these indices may coincide in simpler scenarios, they can diverge in more intricate nonlinear and fully implicit systems~\cite{lamour2012detecting}. Indeed, the index may exhibit local variability, assuming different values across distinct regions and could even remain undefined at singular points~\cite{lamour2012detecting}. Notice that the differentiation index is the most common index used in practice and is typically referred to without any further epithet, \ie{}, ``the'' index. In the following sections, we will delve into the differentiation, tractability, and structural indices, which are the most used in practice. Together with their qualitative definition, this discussion will provide a comprehensive understanding of the mathematical structure of \acp{DAE} and the challenges associated with their resolution.

\paragraph{Differential Index}

Given that a \ac{DAE} encompasses both differentiations and integrations, a potential strategy involves iteratively differentiating the constraints (in semi-explicit \ac{DAE} systems) and substituting them into the differential equations. This process, called \emph{index reduction}, aims to transform the \ac{DAE} into an explicit \ac{ODE} system for all unknowns. The solutions of the \ac{DAE} correspond to those solutions of the resulting \ac{ODE} residing within a subset termed the solution manifold. The number of iterations required for this transformation is termed the differential index of the \ac{DAE}, with the \emph{underlying} \acp{ODE} possessing an index of 0~\cite{mehrmann2015index}.

\begin{example}[Differential Index]
  Consider the following simple examples involving a given smooth function $q(t)$ and the unknown $x$. Then the scalar equation
  %
  \begin{equation}
    x = q(t)
    \label{chap1:eq:dae_index_1}
  \end{equation}
  %
  constitutes a differential index of 1 \ac{DAE}, as it requires one differentiation to yield the \ac{ODE} $x^{\prime} = q^{\prime}(t)$. Similarly, for the system
  %
  \begin{equation}
    \begin{cases}
      x_1 = q(t) \\
      x_2 = x_1^{\prime}
    \end{cases}
    \label{chap1:eq:dae_index_2}
  \end{equation}
  %
  differentiating the first equation leads to $x_2 = x_1^{\prime} = q^{\prime}(t)$, and subsequent differentiation yields $x_2^{\prime} = x_1^{\prime\prime} = q^{\prime\prime}(t)$. Hence, this system possesses a differential index of 2, necessitating two differentiations.
\end{example}

It is crucial to understand that while $n$ initial or boundary value conditions are required to define the solution of a first-order \acp{ODE} of size $n$, the solution of the simple \acp{DAE} in the previous example is solely determined by the right-hand side, necessitating only one consistent initial condition. General \ac{DAE} systems often include \ac{ODE} subsystems, resulting in $k \in [0, n]$ \acp{DOF} for the \ac{DAE} solution. However, determining which $k$ pieces of information are necessary to determine the solution can be challenging. Initial or boundary conditions specified for the \ac{DAE} must be consistent, satisfying both the constraints and potentially the \emph{hidden} (or differentiated) constraints of the system. For instance, in the index-1 system~\eqref{chap1:eq:dae_index_1}, an initial condition must adhere to $x_1(0) = q(0)$. In the case of the index-2 system~\eqref{chap1:eq:dae_index_2}, meeting the additional constraint $x_2(0) = q^{\prime}(0)$ is necessary, along with $x_1(0) = q(0)$.

\paragraph{Tractability Index}

In the semi-explicit \ac{DAE}\eqref{chap1:eq:semiexplicit_dae}, the index is one if the Jacobian matrix $\partial\m{g}/\partial\m{z}$ is non-singular, indicating that one differentiation of the algebraic equation yields $z^{\prime}$. Differential variables $y$ have their derivatives appearing in the equations, while algebraic variables $z$ do not. Notably, algebraic variables may exhibit one less degree of smoothness compared to differential variables. In the general case~\eqref{chap1:eq:dae}, each component of the solution $x$ may consist of a combination of both differential and algebraic components, adding complexity to qualitative analysis and numerical solutions, particularly for high-index problems. While the semi-explicit form simplifies this complexity, it raises the index by one. Moreover, the index may vary depending on a specific solution, as illustrated in the subsequent example.

\begin{example}[Differential Index Dependency on Solution]
  Consider the \ac{DAE} system
  %
  \begin{equation*}
    \begin{cases}
    x_1^{\prime} = x_3 \\
    0 = x_2(1 - x_2) \\
    0 = x_1x_2 + x_3(1 - x_2) - t
    \end{cases}
  \end{equation*}
  %
  The second equation admits two solutions: $x_2 = 0$ and $x_2 = 1$. If $x_2$ is continuous, the system does not transition between these values. For $x_2 = 0$, the system is semi-explicit with an index of 1, while for $x_2 = 1$, the index becomes 2. Unlike the index-1 scenario, no initial value of $x_1$ is necessary. Replacing the algebraic equation involving $x_2$ with $x_2^{\prime} = 0$, leads the index of the modified \ac{DAE} system to depend on the initial condition. Specifically, if $x_2(0) = 1$, the index is 2; otherwise, it remains 1.
\end{example}

The tractability index concept is specifically designed to tackle the challenges posed by high-index \acp{DAE} with multiple regularity regions. It explicitly suggests that the domain of definition of the \ac{DAE} can be divided into maximal regularity regions, each delimited by critical points. Within each regularity region, the \ac{DAE} exhibits a consistent structure, which can be revealed using matrix function sequences formed with permissible projector functions. This construction is governed by constant rank conditions. Critical points arise when this construction process encounters difficulties. A smooth flow and reliable treatment are expected as long as the solution remains within a regularity region. However, crossing or touching a boundary between regions may lead to significant singularities. Essentially, monitoring the structure involves computing an admissible matrix function sequence and monitoring the rank conditions to ensure the stability and reliability of the solution~\cite{lamour2011computational}.

The fundamental idea behind the tractability index concept involves the utilization of derivatives of projectors instead of derivative arrays. This approach offers the advantage of explicitly specifying the smoothness requirements for the inhomogeneity, thereby facilitating the extension of the tractability index to infinite-dimensional systems. However, if numerical computation of the projectors is necessary, challenges may arise in obtaining the derivatives~\cite{mehrmann2015index}. Characterizing the precise relationship between the tractability index and other indices remains a partially open problem. Some partial results have been obtained, as documented in~\cite{campbell1995index, campbell1995solvability, marz2005characterizing}, indicating that, except for variations in smoothness requirements, the tractability index equals the differentiation index.

\paragraph{Structural Index}

A combinatorially oriented index was initially defined for the linear constant coefficient case. Given the linear \acp{DAE} $\m{E}\m{x}^{\prime} = \m{A}\m{x} + \m{f}(t)$, consider the parameter-dependent pencil $(\m{E}(\m{p}), \m{A}(\m{p}))$, obtained by substituting the nonzero elements of $\m{E}$ and $\m{A}$ with independent parameters $\m{p}$. The unique integer that equals the Kronecker index of $(\m{E}(\m{p}), \m{A}(\m{p}))$ for all $\m{p}$ from some open and dense subset of the parameter set is termed the structural index, as discussed in~\cite{pantelides1988consistent, pryce2001simple} in a broader context. For the nonlinear case, a local linearization is typically utilized. Although in~\cite{reissig2000differential} it has demonstrated that the differentiation index and the structural index can differ arbitrarily, Pandelide's algorithm, outlined in ~\cite{pantelides1988consistent}, is widely employed in applications. For instance, in~\cite{unger1995structural}, combinatorial information is used to analyze which equations should be differentiated and to introduce additional variables for index reduction~\cite{mattsson1993index}. However, a thorough analysis of when this approach is fully justified has only been provided in a few specific cases, refer to~\cite{mehrmann2015index} for more details.

\subsection{Hessenberg Form of Differential-Algebraic Equations}

The general \ac{DAE} system~\eqref{chap1:eq:dae} may encompass ill-defined mathematical problems and situations that challenge direct discretization methods (refer to the Numerical Solution section). However, many high-index problems encountered in practice can be articulated as a blend of more constrained \ac{ODE} structures intertwined with constraints. In such systems, the algebraic and differential variables are explicitly identified for higher-index DAEs as well, and the algebraic variables may all be eliminated using the same number of differentiations. These are called Hessenberg forms of the \acp{DAE} and are given as follows~\cite{brenan1995numerical}, and will be later used in the following sections and chapters.

\paragraph{Hessenberg Index-1}

The Hessenberg index-1 \ac{DAE} system is expressed as
%
\begin{equation*}
  \begin{cases}
    \m{x}^{\prime} = \m{f}(\m{x}, \m{z}, t) \\
    \m{0} = \m{g}(\m{x}, \m{z}, t)
  \end{cases}
\end{equation*}
%
where the Jacobian $\m{Jg}_\m{z}$ is presumed non-singular for all $t$. This formulation mirrors the semi-explicit index-1 \ac{DAE} system discussed earlier. Semi-explicit index-1 \acp{DAE} bear close resemblance to implicit \acp{ODE}. While theoretically one can solve for $z$ in the algebraic equation (via the implicit function theorem), and then substitute it into the differential equation to derive the underlying \ac{ODE} in $x$ (though uniqueness isn't guaranteed), this method is not universally recommended for numerical solutions due to various considerations.

\paragraph{Hessenberg Index-2}

The Hessenberg index-2 \ac{DAE} system is expressed as
%
\begin{equation*}
  \begin{cases}
    \m{x}^{\prime} = \m{f}(\m{x}, \m{z}, t) \\
    \m{0} = \m{g}(\m{x}, t)
  \end{cases}
\end{equation*}
%
where the product of Jacobians $\m{Jg}_\m{y}\m{Jf}_\m{z}$ is assumed non-singular for all $t$. Notably, the algebraic variable $y$ is absent from the second equation. This constitutes a pure index-2 \ac{DAE}, where all algebraic variables function as index-2 variables. An instance arising from modeling incompressible fluid flow through discretized Navier-Stokes equations is detailed in~\cite{ascher1998computer}.

\paragraph{Hessenberg Index-3}

The Hessenberg index-3 \ac{DAE} system is expressed as
%
\begin{equation*}
  \begin{cases}
    \m{x}^\prime = \m{f}(\m{x}, \m{y}, \m{z}, t) \\
    \m{y}^\prime = \m{g}(\m{x}, \m{y}, t) \\
    \m{0}        = \m{h}(\m{y}, t)
  \end{cases}
\end{equation*}
%
where the product $\m{Jh}_{\m{y}} \, \m{Jg}_{\m{x}} \, \m{Jf}_{\m{z}}$ is non-singular. The index of a Hessenberg DAE is found, as in the general ∂y ∂x ∂z
case, by differentiation. However, only algebraic constraints must be differentiated~\cite{ascher1991projected}. These index-3 \ac{DAE} systems are a common structure encountered in practice, particularly in the context of \ac{MBD}, \acp{TPPC}, and many other complex systems.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Solution Methods}

Approaches for numerically solving \acp{DAE} can broadly be categorized into two classes: (\emph{i}) direct discretization of the given system and (\emph{ii}) methods involving index reduction (\eg{}, a form of reformulation) along with discretization. While direct discretization is preferred due to its simplicity and efficiency, index reduction approaches are popular for their versatility, especially when dealing with higher-index \acp{DAE}.

\subsection{Numerical Direct Discretization}

Direct discretization is favored because index reduction can be both time and computationally expensive, requiring more user input and intervention. However, direct discretization is primarily effective for index-1, index-2 Hessenberg, and index-3 Hessenberg \ac{DAE} systems. Thankfully, many practical \acp{DAE} are either index-1 or can be transformed into simple combinations of Hessenberg systems. Despite this, challenges may arise, and even for these restricted classes of problems, direct applications of numerical \ac{ODE} methods may not always yield satisfactory results. For \acp{DAE} with an index greater than two, employing index-reduction techniques to solve the problem in a lower-index form is often the best approach.

Considerations akin to singularly perturbed \ac{ODE} systems arise in cases like

\begin{equation}
  \begin{cases}
  \m{y}^{\prime} = \m{f}(\m{y}, \m{z}, t) \\
  \varepsilon \m{z}^{\prime} = \m{g}(\m{y}, \m{z}, t),
  \end{cases}
  \label{chap1:eq:singularly_perturbed}
\end{equation}

where $\varepsilon$ is a small parameter. Setting $\varepsilon = 0$ reduces equation~\eqref{chap1:eq:singularly_perturbed} to the \ac{DAE}~\eqref{chap1:eq:semiexplicit_dae}. Given the system's stiffness for small $\varepsilon$, methods tailored for stiff \acp{ODE}, such as \ac{BDF} and Radau collocation methods, are natural choices for direct discretization of the limit \ac{DAE} and for handling \acp{DAE} of the form~\eqref{chap1:eq:dae} in general.

\paragraph{Backward Euler Method}

The concept behind direct discretization is straightforward: approximate $\m{x}$ and $\m{x}^{\prime}$ using a discretization formula such as multistep methods or Runge-Kutta methods. To illustrate direct discretization, let's take the backward Euler method, the simplest method possessing the stiff decay property. By applying the backward difference formula to $\m{x}^{\prime}$ in equation~\eqref{chap1:eq:dae}, we arrive at a system of $N$ nonlinear equations
%
\begin{equation*}
  \m{F}\left(\m{x}_n, \dfrac{\m{x}_n - \m{x}_{n-1}}{h_n}, t_n\right) = \m{0} \quad \text{for} \quad n = 1, 2, \dots
\end{equation*}
%
where $\m{x}_n$ represents the approximation of $\m{x}(t_n)$, $h_n = t_n - t_{n-1}$ denotes the time step, and $t_n$ are the time points. Solving this nonlinear equation system recursively yields a numerical solution for equation~\eqref{chap1:eq:dae}. This method is effective for index-1 \acp{DAE} and particularly suitable for stiff index-1 \acp{DAE} and stiff \acp{ODE}.

However, for higher-index \acp{DAE}, including other methods, this simple approach may not suffice. In some cases, even seemingly straightforward higher-index \ac{DAE} systems with well-defined and stable solutions can pose challenges, rendering methods like the backward Euler method and other multistep and Runge-Kutta methods unstable or inapplicable. Refer to Example 10.1 in~\cite{ascher1998computer} for an illustration of such cases.

Moreover, practical difficulties may arise during the solution of the nonlinear system (13) for $\m{x}_n$ given $\m{x}_{n-1}$. Typically, iterative numerical methods like the Newton method are employed to obtain the solution. Due to these technical challenges, direct discretization of fully implicit \acp{DAE} with an index higher than one is generally discouraged. However, for fully implicit index-1 and semi-explicit index-2 \acp{DAE}, the Backward Euler method has been shown to be stable, convergent, and first-order accurate. For detailed discussions and convergence results, see~\cite{brenan1995numerical, hairer1999stiff}.

\paragraph{Backward Differentiation Formula and Linear Multistep Methods}

While Euler is merely a first-order method, achieving greater accuracy without reducing step size necessitates higher-order methods. The constant step-size \ac{BDF} method applied to a general nonlinear \ac{DAE} of the form~\eqref{chap1:eq:dae} is given by:
%
\begin{equation*}
  \m{F}\left(\m{x}_n, \dfrac{1}{\beta_0 h} \sum_{j=0}^{k} \alpha_j x_{n-j}, t_n\right) = \m{0}
\end{equation*}
%
where $\beta_0$ and $\alpha_j$ for $j = 0, 1, \dots, k$ are the coefficients of the \ac{BDF} method. The $k$-step \ac{BDF} method of fixed step size $h$ has been shown to be convergent of order $\mathcal{O}(h^k)$ under certain conditions. Similar convergence results have been established for general linear multistep methods, provided their coefficients satisfy a set of order conditions, including those unique to \acp{DAE}. \ac{BDF} methods meet these additional requirements. Refer to~\cite{brenan1995numerical} for more information.

\paragraph{Radau Collocation and Implicit Runge-Kutta Methods}

The $s$-stage implicit Runge-Kutta method applied to a general nonlinear \ac{DAE} of the form~\eqref{chap1:eq:dae} is given by:
%
\begin{equation*}
  \begin{array}{l}
      \m{F}\left(\m{X}_{ni}, \m{K}_{ni}, t_{n-1}+c_i h\right) = \m{0} \\[1.0em]
      \m{X}_{ni} = \m{x}_{n-1} + h\displaystyle\sum_{j=1}^s a_{ij}\m{K}_j \\[0.25em]
      \m{x}_n = \m{x}_{n-1} + h\displaystyle\sum_{i-1}^s b_i \m{K}_{ni}
  \end{array}
\end{equation*}
%
where $c_i$, $a_{ij}$, $b_i$ for $i$, $j = 1, 2, \dots, s$ are the coefficients of the Runge-Kutta method, with the additional assumption that the matrix $A = (a_{ij})$ is non-singular.

For the semi-explicit \ac{DAE}~\eqref{chap1:eq:semiexplicit_dae}, the formula for the internal stages reads:
%
\begin{equation*}
  \begin{array}{l}
  \m{K}_{ni} = \m{f}\left(\m{Y}_{ni}, \m{Z}_{ni}, t_{n-1}+c_i h\right) \\
  \m{Y}_{ni} = \m{y}_{n-1} + h\displaystyle\sum_{j=1}^s a_{ij}\m{K}_j \\
  \m{g}\left(\m{Y}_{ni}, \m{Z}_{ni}, t_{n-1}+c_i h\right) = \m{0}
  \end{array}
\end{equation*}
%
for $i = 1, 2, \dots, s$. It is possible to avoid the quadrature step for the algebraic variables $z$ by employing stiffly accurate methods, such as Runge-Kutta methods satisfying $b_i = a_{si}$ for $i = 1, 2, \dots, s$. These methods have additional order conditions for attaining order greater than 2, even for semi-explicit index-1 \acp{DAE}, as seen with Runge-Kutta methods. Refer to~\cite{brenan1995numerical} for further details.

It is important to acknowledge that implementing direct discretization methods for \acp{DAE} encounters practical challenges. These include obtaining a consistent set of initial conditions, addressing the ill-conditioning of the iteration matrix, and managing error estimation and stepsize control for index-2 Hessenberg \acp{DAE}.

For certain special classes of \acp{DAE}, like semi-explicit \acp{DAE} in Hessenberg form and \acp{ODE} on manifolds, especially those encountered in \ac{MB} mechanics, highly efficient and robust numerical methods called stabilized or projected methods are available. The fundamental approach involves discretizing the differential equations using a suitable numerical \ac{ODE} method, followed by a post-stabilization or coordinate projection step to align the numerical solution with the constraint. For further details, refer to~\cite{eichsoellner1998numerical}.

For a comprehensive understanding of numerical aspects related to \acp{DAE}, refer to~\cite{ascher1998computer, brenan1995numerical, hairer1999stiff}.

\subsubsection{Software for Differential-Algebraic Equations Numerical Solution}

Several software packages are available for solving \acp{DAE}, including both general-purpose and specialized tools. Some of the most widely used software packages for \ac{IVP} and \ac{BVP} \acp{DAE}'s numerical solutions are listed here below.
%
\begin{itemize}
  \setlength{\itemsep}{0.0em}
  \item DASSL: Developed by Linda Petzold, DASSL employs \ac{BDF} formulas to solve general index-1 \acp{DAE}. Versions catering to large-scale problems (DASPK) and sensitivity analysis are also available. Some later versions of DASPK can handle Hessenberg index-2 \acp{DAE}. Additionally, there is DASPKADJOINT, which implements the adjoint method for sensitivity analysis of \ac{DAE} systems~\cite{brenan1995numerical}.
  \item RADAU5: A code by \citet{hairer1999stiff} based on the 3-stage Radau collocation method. It addresses \acp{DAE} of the form $\m{M}\m{x}^{\prime} = \m{f}(\m{x}, t)$, where $\m{M}$ is a constant, possibly singular, square matrix. RADAU5 accommodates problems up to index 3, with the higher-index variables requiring user identification.
  \item IDA: A component of the SUNDIALS (SUite of Nonlinear and DIfferential/ALgebraic equation Solvers) software package developed by Radu Serban and Alan C. Hindmarsh at Lawrence Livermore National Laboratory, USA~\cite{hindmarsh2005sundials, gardner2022sundials}. IDA, written in C, solves nonlinear \acp{DAE} and is derived from the FORTRAN package DASPK. There is also CPODES, a related code for solving \acp{ODE} with invariants.
  \item DAEPACK: A software library created by Paul I. Barton and his team at \ac{MIT}~\cite{tolsma2000daepack}. While its name stands for Differential-Algebraic Equation Package, DAEPACK's capabilities extend beyond \ac{DAE} analysis, encompassing both symbolic and numerical components for modeling and general numerical computations.
  \item COLDAE: Developed by \citet{ascher1994collocation}, COLDAE employs projected Gauss collocation to solve boundary value problems for semi-explicit index-2 \acp{DAE}.
\end{itemize}

\subsection{Index Reduction Methods}

For higher-index \acp{DAE}, direct discretization methods may not be the most efficient or effective. In such cases, index reduction methods are preferred. These methods involve transforming the \ac{DAE} into a lower-index form, which can then be solved using either direct discretization methods or standard \ac{ODE} integration techniques with the aid of projection or stabilization techniques. The most common methods include the \emph{differential index} reduction. The differential index~\cite{campbell1995index, campbell1995highindex} is the minimum number of differentiation times required to transform a \ac{DAE} system into an explicit \ac{ODE} system and is used throughout this work. Generally, the differential index reduction process involves differentiating the algebraic constraints and substituting them into the differential equations. This process is repeated until the \ac{DAE} is transformed into an explicit \ac{ODE} system. The number of iterations required to achieve this transformation is termed the differential index of the \ac{DAE}, with \acp{ODE} possessing an index of 0. This method is particularly useful for high-index \acp{DAE} and is widely employed in software packages like \textsc{ModelingToolkit}, \textsc{Modelica}, and \textsc{Modelica}-based tools like \textsc{OpenModelica}.

As an expert reader may have noticed, the differential index reduction method is closely related to the aforementioned symbolic computation techniques. Symbolic computation tools like \textsc{Maple}, \textsc{Mathematica}, and \textsc{SymPy} can be used to perform the differentiation and substitution steps required to reduce the index of a \ac{DAE}. These tools can also be used to derive the Jacobian of the \ac{DAE} system, which is essential for solving the \ac{DAE} system using numerical methods. Nonetheless, in a handful of cases, symbolic computation tools can also be used to derive the solution of the \ac{DAE} system, which can then be used to validate the numerical solution obtained using numerical methods and assess the accuracy of the numerical solution and the numerical method used. However, it is important to note that symbolic computation tools are not always the most efficient or effective tools for solving \acp{DAE}, especially for large-scale or complex \acp{DAE}. Indeed, the computational cost of symbolic computation tools can be prohibitive for large-scale or complex \acp{DAE}, and the numerical solution of the \ac{DAE} system using numerical methods may be preferred.

There exist several techniques for reducing the index of a \ac{DAE} system, the most common of which is the Pantelides algorithm~\cite{pantelides1988consistent}, which falls under the umbrella of the \ac{SA} methods. Recent advances in the field of symbolic computation have led to the development of a broader theory by John Pryce and his collaborators~\cite{pryce1998solving}. This theory provides a systematic approach to index reduction and the solution of \acp{DAE} using Taylor series, and which has been proven in~\cite{pryce2001simple} to be a generalization of the Pantelides algorithm. The theory has been implemented in the software package \textsc{DAETS}~\cite{nedialkov2007solvingI, nedialkov2007solvingII, nedialkov2008solvingIII}, which is particularly effective in solving \emph{very} high-index \acp{DAE} and has been successfully applied to a wide range of problems in engineering and science.

Another approach to index reduction is the so-called \emph{projector-based analysis}, which is thoroughly presented in \citet{lamour2013differential, marz2014differential}. This method involves finding a projector that allows for the separation of the differential and algebraic components of the \ac{DAE} system. The projector is then used to transform the \ac{DAE} system into a lower-index form, which can then be solved using numerical methods. This procedure is repeated point-wise during the numerical solution of the \ac{DAE} system, ensuring that the changes in the \acp{DAE} structure are accounted for at each time step. Special continuous projector-valued functions can be used to ensure that it is possible to reutilize the same projector in a wider range of time steps, thus reducing the computational cost of the method. Notice that in the case of nonlinear \acp{DAE} the projector-based analysis is typically performed pointwise, which means that the algorithmic steps of the computation of admissible nullspace projectors are performed at each time step. This is necessary to ensure that the changes in the \ac{DAE} structure are accounted for at each time step~\cite{lamour2011computational, lamour2012detecting}.

% TRactability già introdotto
Notably, the projector-based analysis relies on \emph{tractability index}~\cite{mehrmann2015index}. The tractability index concept includes the opinion that a more general \acp{DAE} may be constituted by several so-called regularity regions. The definition domain of the \acp{DAE} decomposes into maximal regularity regions which are bordered by critical points. On each regularity region, the \acp{DAE} show a uniform structure which can be uncovered using matrix function sequences formed with admissible projector functions. The construction is supported by constant rank conditions. Points where construction fails are critical ones. A smooth flow and also reliable treatment can be expected if the solution stays within a regularity region. Crossing or touching a border may yield strong singularities. In essence, to monitor the structure one has to compute an admissible matrix function sequence and thereby monitor the rank conditions~\cite{lamour2011computational}. In all works presented by the group of Roswitha M\"{a}rz, there is little mention of the use of symbolic computation tools that could be used to derive the projector functions and the matrix function sequences. This is likely since the projector functions and the matrix function sequences are typically derived using numerical factorization methods with a coupled automatic differentiation method to \dots. However, it is possible that symbolic computation tools could be used to derive the projector functions and the matrix function sequences, especially for simple or small-scale \acp{DAE}.

\subsection{Software for Index Reduction of Differential-Algebraic Equations}

Following this brief introduction of the most widely recognized index reduction algorithms, we provide an overview of the specific index reduction algorithms found in current state-of-the-art software. The following list is limited to the most prominent solutions dedicated to dynamic system modeling and simulation that are widely adopted in both academia and industry.
%
\begin{itemize}
  \setlength{\itemsep}{0.0em}
  \item \Matlab{} uses the Pantelides algorithm~\cite{pantelides1988consistent} to reduce the \acp{DAE} to index-1. Alternatively, or if the latter fails, the more reliable but slower Gaussian elimination algorithm can be employed to obtain an index-0 \ac{DAE} system~\cite{matlab}.
  \item \textsc{Modelica}-based software~\cite{mattsson1997modelica, mattsson1998physical} and \textsc{ModelingToolkit}~\cite{modelingtoolkit} employ the Pantelides algorithm~\cite{pantelides1988consistent} along with the dummy derivatives method~\cite{mattsson1993index} to automatically perform the reduction to index-1 \acp{DAE}.
  \item \Mathematica{} offers a comprehensive suite of index reduction algorithms~\cite{mathematica}. It can reduce the index of \acp{DAE} using the Pantelides~\cite{pantelides1988consistent} or structural matrix~ \cite{unger1995structural, chowdhry2004symbolic} methods. Additionally, it implements dummy derivatives~\cite{mattsson1993index} and projection methods for taking hidden constraints into account during numerical integration.
  \item \Maple{} performs symbolic index reduction within the \texttt{dsolve} function~\cite{maple}. However, the implemented algorithms are not documented or referenced. They are likely to be based on the projection method outlined in~\cite{shmoylova2013simplification}. Notably, the patents by the same authors~\cite{postma2012exact, shmoylova2012method, postma2015exact} discuss techniques for eliminating isolated parameters, extracting parameter sub-expressions from \acp{DAE}, and establishing minimal disconnected clusters of parameter sub-expressions. We would like to point out that this information is to be taken with caution, as \Maple{} does not provide specific references on this topic.
\end{itemize}
%
All the showcased software solutions offer integrators for index-0 and index-1 \acp{DAE}. Depending on the system's stiffness, users can select the most appropriate algorithm for numerically integrating the reduced-index system.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %